{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karen-pal/notebook/blob/main/Image_To_Text_(LLM_%2B_CLIP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image To Text (LLM + CLIP)\n",
        "\n",
        "Notebook by Katherine Crowson (https://twitter.com/RiversHaveWings)\n",
        "\n",
        "This notebook uses reinforcement learning to fine-tune a large language model ([Pythia 160M](https://github.com/EleutherAI/pythia) by default) to interpret a single image according to a [CLIP](https://arxiv.org/abs/2103.00020) based image/text matching loss."
      ],
      "metadata": {
        "id": "tdD_I-GpO1VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 { display-mode: \"form\" }\n",
        "\n",
        "# Copyright 2024 Katherine Crowson\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "XegPrTKPPL4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0ANc8LxOwM_"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "!pip install open_clip_torch peft"
      ],
      "metadata": {
        "id": "ad5YzQakPQI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import textwrap\n",
        "\n",
        "from google.colab import files\n",
        "import open_clip\n",
        "import peft\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "KVS9Xe35P5WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define necessary functions\n",
        "\n",
        "print = tqdm.external_write_mode()(print)\n",
        "\n",
        "\n",
        "def endless_range(start=0, step=1):\n",
        "    \"\"\"An endless range generator.\"\"\"\n",
        "    i = start\n",
        "    while True:\n",
        "        yield i\n",
        "        i += step\n",
        "\n",
        "\n",
        "def logp_completion(logits, tokens, mask):\n",
        "    \"\"\"Compute the log probabilities of completions given their prompts.\n",
        "\n",
        "    Args:\n",
        "        tokens: The tokens input to the model. Shape: (..., T).\n",
        "        logits: The logits output from the model. Shape: (..., T, V).\n",
        "        mask: A mask indicating which tokens should be included in the log probabilities. It should\n",
        "            exclude prompt tokens and padding tokens. Shape: (..., T).\n",
        "\n",
        "    Returns:\n",
        "        The log probabilities of the completions given their prompts. Shape: (...).\n",
        "    \"\"\"\n",
        "    logits = F.log_softmax(logits, dim=-1)\n",
        "    logp_tokens = logits[..., :-1, :].gather(-1, tokens[..., 1:, None])[..., 0]\n",
        "    return torch.sum(logp_tokens * mask[..., 1:], dim=-1)\n"
      ],
      "metadata": {
        "id": "IZlXDvbKQOZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload image\n",
        "\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) == 1, \"Please upload exactly one image.\"\n",
        "image = Image.open(list(uploaded.keys())[0])"
      ],
      "metadata": {
        "id": "wfXwQZdwQiY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set parameters { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Generations from the LLM will be prefixed by the prompt:\n",
        "prompt = \"The theme of this image is\"  #@param {type: 'string'}\n",
        "\n",
        "#@markdown The number of tokens to sample from the LLM:\n",
        "length = 50  #@param {type: 'integer'}\n",
        "\n",
        "#@markdown The batch size:\n",
        "batch_size = 64  #@param {type: 'integer'}\n",
        "\n",
        "#@markdown The strength of the KL divergence penalty vs the original LLM:\n",
        "#@markdown <br><small>The KL divergence penalty specifies the rate at which the optimizer will trade off a decrease in the angle (in radians) between the CLIP text and image embeddings and a decrease in the KL divergence between the model and the reference model.</small>\n",
        "kl_weight = 4e-3  #@param {type: 'number'}\n",
        "\n",
        "#@markdown The temperature at which to sample from the LLM:\n",
        "temperature = 0.9  #@param {type: 'number'}\n"
      ],
      "metadata": {
        "id": "6_k53cVFyAQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load models\n",
        "\n",
        "clip_name = \"ViT-L-14-336\"\n",
        "clip_pretrained = \"openai\"\n",
        "model_name = \"EleutherAI/pythia-160m-deduped\"\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Load CLIP\n",
        "clip_tokenizer = open_clip.get_tokenizer(clip_name)\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    clip_name, pretrained=clip_pretrained, device=device\n",
        ")\n",
        "clip_model.eval().requires_grad_(False)\n",
        "\n",
        "# Load language model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map={\"\": device}\n",
        ")\n",
        "\n",
        "# Prepare LoRA\n",
        "peft_config = peft.LoraConfig(\n",
        "    peft.TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=32,\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.0,\n",
        "    target_modules=[\n",
        "        # For NeoX and Pythia\n",
        "        \"attention.query_key_value\",\n",
        "        \"attention.dense\",\n",
        "        \"mlp.dense_h_to_4h\",\n",
        "        \"mlp.dense_4h_to_h\",\n",
        "        # For Llama and Mistral 7B\n",
        "        # \"self_attn.q_proj\",\n",
        "        # \"self_attn.k_proj\",\n",
        "        # \"self_attn.v_proj\",\n",
        "        # \"self_attn.o_proj\",\n",
        "        # \"mlp.gate_proj\",\n",
        "        # \"mlp.up_proj\",\n",
        "        # \"mlp.down_proj\",\n",
        "    ],\n",
        ")\n",
        "model = peft.get_peft_model(model, peft_config)\n",
        "\n",
        "# Finish preparing model\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "model.train()\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "6z9MGEK_wIM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optimize the LLM\n",
        "\n",
        "# Settings\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# Prepare the input image\n",
        "image_for_clip = preprocess(image).unsqueeze(0).to(device)\n",
        "with torch.cuda.amp.autocast():\n",
        "    image_embed = clip_model.encode_image(image_for_clip).float()\n",
        "\n",
        "# Prepare the prompt\n",
        "inputs = tokenizer([prompt] * batch_size, return_tensors=\"pt\").to(device)\n",
        "input_len = inputs.input_ids.shape[1]\n",
        "logp_mask = torch.tensor(\n",
        "    [[False] * input_len + [True] * length] * batch_size, device=device\n",
        ")\n",
        "\n",
        "# Optimize the LLM\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
        "\n",
        "try:\n",
        "    for i in tqdm(endless_range()):\n",
        "        # Generate a batch of samples from the model\n",
        "        model.eval()\n",
        "        tokens = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            do_sample=True,\n",
        "            min_new_tokens=length,\n",
        "            max_new_tokens=length,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            temperature=temperature,\n",
        "            top_k=0,\n",
        "        )\n",
        "\n",
        "        # Get the logits of the samples from the model and the reference model\n",
        "        attention_mask = torch.cat(\n",
        "            (inputs.attention_mask, torch.ones_like(tokens[:, input_len:])), dim=1\n",
        "        )\n",
        "        with torch.no_grad(), model.disable_adapter():\n",
        "            outputs_ref = model(tokens, attention_mask=attention_mask, use_cache=False)\n",
        "        model.train()\n",
        "        outputs = model(tokens, attention_mask=attention_mask, use_cache=False)\n",
        "\n",
        "        # Compute the log probability of the samples under the model and the reference model\n",
        "        logp = logp_completion(outputs.logits / temperature, tokens, logp_mask)\n",
        "        logp_ref = logp_completion(outputs_ref.logits / temperature, tokens, logp_mask)\n",
        "\n",
        "        # Compute the CLIP loss\n",
        "        texts = [tokenizer.decode(t, skip_special_tokens=True) for t in tokens]\n",
        "        clip_tokens = clip_tokenizer(texts).to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            text_embeds = clip_model.encode_text(clip_tokens).float()\n",
        "        cost_clip = torch.cosine_similarity(text_embeds, image_embed, dim=-1).arccos()\n",
        "\n",
        "        # Compute the KL penalty\n",
        "        cost_kl = logp.detach() - logp_ref\n",
        "\n",
        "        # REINFORCE\n",
        "        cost = cost_clip + kl_weight * cost_kl\n",
        "        baseline = (cost.sum() - cost) / (cost.numel() - 1)\n",
        "        box = torch.exp(logp - logp.detach())\n",
        "        loss = torch.mean(box * cost + (1 - box) * baseline)\n",
        "\n",
        "        # Update the model\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Print statistics and the best sample in the batch\n",
        "        grad_norm = torch.cat(\n",
        "            [p.grad.flatten() for p in model.parameters() if p.grad is not None]\n",
        "        ).norm()\n",
        "        print(\n",
        "            f\"step: {i}, loss: {loss.item():g}, clip: {cost_clip.mean().item():g}, kl: {cost_kl.mean().item():g}, grad: {grad_norm.item():g}\"\n",
        "        )\n",
        "        best_text = texts[torch.argmin(cost).item()]\n",
        "        print(textwrap.fill(best_text, width=80))\n",
        "        print()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "metadata": {
        "id": "3QwdDrKXzlKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}